{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE4CNeOXTBxJVK6svHhslW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhanuPrakash-16/GAN_for_images_Project-3/blob/main/GAN_for_images_Project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV9sMmy8OR1D"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 1: Global Configuration\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "\n",
        "CONFIG = {\n",
        "    \"img_size\": 64,\n",
        "    \"channels\": 3,\n",
        "    \"latent_dim\": 100,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 50,\n",
        "    \"lr\": 0.0002,\n",
        "    \"beta_1\": 0.5,\n",
        "    \"beta_2\": 0.999,\n",
        "7\n",
        "    \"data_dir\": \"/content/drive/MyDrive/DCGANProject/ExampleDataDir/PartialPlantExamples\",\n",
        "    \"checkpoint_dir\": \"/content/drive/MyDrive/DCGANProject/checkpoints\",\n",
        "    \"sample_dir\": \"/content/drive/MyDrive/DCGANProject/samples\",\n",
        "    \"log_dir\": \"/content/drive/MyDrive/DCGANProject/logs\",\n",
        "}\n",
        "\n",
        "for d in [\"checkpoint_dir\", \"sample_dir\", \"log_dir\"]:\n",
        "    os.makedirs(CONFIG[d], exist_ok=True)\n",
        "CONFIG\n"
      ],
      "metadata": {
        "id": "K8GVKiLSOYJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 2: Data Configuration Loader (FIXED)\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "os.makedirs(r\"/content/drive/MyDrive/DCGANProject\\configs\", exist_ok=True)\n",
        "\n",
        "DATA_CONFIG = {\n",
        "    \"image_size\": 64,\n",
        "    \"classes\": [\n",
        "        \"healthy\",\n",
        "        \"early_blight\",\n",
        "        \"late_blight\",\n",
        "        \"rust\",\n",
        "        \"mildew\",\n",
        "        \"mosaic\"\n",
        "    ],\n",
        "    \"splits\": [\"train\", \"validation\", \"test\"]\n",
        "}\n",
        "\n",
        "with open(r\"/content/drive/MyDrive/DCGANProject\\configs\\data_config.yaml\", \"w\") as f:\n",
        "    yaml.dump(DATA_CONFIG, f)\n",
        "\n",
        "print(\"data_config.yaml created successfully inside configs/\")\n",
        "\n",
        "# A YAML file is a human-readable configuration file used to store settings,\n",
        "# parameters, and structured data without writing code.\n",
        "# YAML is used to keep paths, hyperparameters, and experiment settings cleanly\n",
        "# separated from Python code.\n"
      ],
      "metadata": {
        "id": "EHiVUNVSOaB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 3: Dataset Loader (FINAL FIX)\n",
        "# =====================================\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "DATASET_ROOT = r\"/content/drive/MyDrive/DCGANProject/ExampleDataDir/PartialPlantExamples\"\n",
        "\n",
        "def load_plantvillage_dataset(img_size=64, batch_size=64):\n",
        "    ds = image_dataset_from_directory(\n",
        "        DATASET_ROOT,\n",
        "        label_mode=None,              # GAN ‚Üí no labels\n",
        "        image_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Normalize images to [-1, 1] for DCGAN\n",
        "    ds = ds.map(\n",
        "        lambda x: (tf.cast(x, tf.float32) / 127.5) - 1.0,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print (\"Data Set Root\", DATASET_ROOT)\n",
        "\n",
        "def count_images_in_directory(DATASET_ROOT):\n",
        "    valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(DATASET_ROOT):\n",
        "        count += sum(f.lower().endswith(valid_ext) for f in files)\n",
        "    return count\n",
        "\n",
        "total_images = count_images_in_directory(DATASET_ROOT)\n",
        "print(\"Total images:\", total_images)\n",
        "\n"
      ],
      "metadata": {
        "id": "86yZjoCDObrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 4: Generator Model (FIXED)\n",
        "# =====================================\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_generator(latent_dim=100):\n",
        "    model = models.Sequential(name=\"Generator\")\n",
        "\n",
        "    model.add(layers.Dense(4 * 4 * 512, input_dim=latent_dim))\n",
        "    model.add(layers.Reshape((4, 4, 512)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # 4x4 ‚Üí 8x8\n",
        "    model.add(layers.Conv2DTranspose(256, 4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # 8x8 ‚Üí 16x16\n",
        "    model.add(layers.Conv2DTranspose(128, 4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # 16x16 ‚Üí 32x32\n",
        "    model.add(layers.Conv2DTranspose(64, 4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # 32x32 ‚Üí 64x64 ‚úÖ (MISSING EARLIER)\n",
        "    model.add(layers.Conv2DTranspose(32, 4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # Final RGB image\n",
        "    model.add(layers.Conv2DTranspose(\n",
        "        3, kernel_size=3, activation=\"tanh\", padding=\"same\"\n",
        "    ))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Sanity check\n",
        "generator = build_generator()\n",
        "generator.summary()\n"
      ],
      "metadata": {
        "id": "d7PdSlqrOdUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 5: Discriminator Model (With Summary)\n",
        "# =====================================\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_discriminator(img_size=64, channels=3):\n",
        "    model = models.Sequential(name=\"Discriminator\")\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        64,\n",
        "        kernel_size=4,\n",
        "        strides=2,\n",
        "        padding=\"same\",\n",
        "        input_shape=(img_size, img_size, channels)\n",
        "    ))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, 4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, 4, strides=2, padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ====== Instantiate & Print Summary ======\n",
        "discriminator = build_discriminator(img_size=64, channels=3)\n",
        "discriminator.summary()\n"
      ],
      "metadata": {
        "id": "Ak9hkcz2OfJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 6: DCGAN Wrapper\n",
        "# =====================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "class DCGAN(tf.keras.Model):\n",
        "    def __init__(self, generator, discriminator, config):\n",
        "        super().__init__()\n",
        "        self.G = generator\n",
        "        self.D = discriminator\n",
        "        self.latent_dim = config[\"latent_dim\"]\n",
        "\n",
        "        self.d_optimizer = optimizers.Adam(\n",
        "            config[\"lr\"], beta_1=config[\"beta_1\"]\n",
        "        )\n",
        "        self.g_optimizer = optimizers.Adam(\n",
        "            config[\"lr\"], beta_1=config[\"beta_1\"]\n",
        "        )\n",
        "\n",
        "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    def compile(self):\n",
        "        super().compile()\n",
        "\n",
        "# =====================================\n",
        "# Cell 7: Training Utilities\n",
        "# =====================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def smooth_labels(y, smooth_factor=0.1):\n",
        "    return y - smooth_factor + np.random.random(y.shape) * smooth_factor\n",
        "\n",
        "def add_noise(images, std=0.05):\n",
        "    return images + tf.random.normal(tf.shape(images), mean=0.0, stddev=std)"
      ],
      "metadata": {
        "id": "y6Z9y2vqOgzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 8: DCGAN Training Logic (FINAL VERSION)\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Single training step\n",
        "# ------------------------------------------------\n",
        "def train_step(dcgan, real_images):\n",
        "    batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "    # Sample latent noise\n",
        "    noise = tf.random.normal((batch_size, dcgan.latent_dim))\n",
        "\n",
        "    # =========================\n",
        "    # 1Ô∏è‚É£ Train Discriminator\n",
        "    # =========================\n",
        "    with tf.GradientTape() as d_tape:\n",
        "        fake_images = dcgan.G(noise, training=True)\n",
        "\n",
        "        real_pred = dcgan.D(real_images, training=True)\n",
        "        fake_pred = dcgan.D(fake_images, training=True)\n",
        "\n",
        "        # Label smoothing\n",
        "        real_labels = tf.ones((batch_size, 1)) * 0.9\n",
        "        fake_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        d_loss_real = dcgan.loss_fn(real_labels, real_pred)\n",
        "        d_loss_fake = dcgan.loss_fn(fake_labels, fake_pred)\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "    d_grads = d_tape.gradient(d_loss, dcgan.D.trainable_variables)\n",
        "    dcgan.d_optimizer.apply_gradients(\n",
        "        zip(d_grads, dcgan.D.trainable_variables)\n",
        "    )\n",
        "\n",
        "    # =========================\n",
        "    # 2Ô∏è‚É£ Train Generator\n",
        "    # =========================\n",
        "    with tf.GradientTape() as g_tape:\n",
        "        fake_images = dcgan.G(noise, training=True)\n",
        "        fake_pred = dcgan.D(fake_images, training=False)\n",
        "\n",
        "        g_labels = tf.ones((batch_size, 1))\n",
        "        g_loss = dcgan.loss_fn(g_labels, fake_pred)\n",
        "\n",
        "    g_grads = g_tape.gradient(g_loss, dcgan.G.trainable_variables)\n",
        "    dcgan.g_optimizer.apply_gradients(\n",
        "        zip(g_grads, dcgan.G.trainable_variables)\n",
        "    )\n",
        "\n",
        "    return d_loss, g_loss\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Full training loop\n",
        "# ------------------------------------------------\n",
        "def train_dcgan(dcgan, train_dataset, epochs, checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    history = []\n",
        "\n",
        "    print(\"\\nüöÄ DCGAN TRAINING STARTED\")\n",
        "    print(\"=\" * 75)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        d_losses = []\n",
        "        g_losses = []\n",
        "\n",
        "        for real_images in train_dataset:\n",
        "            d_loss, g_loss = train_step(dcgan, real_images)\n",
        "            d_losses.append(d_loss.numpy())\n",
        "            g_losses.append(g_loss.numpy())\n",
        "\n",
        "        mean_d = float(np.mean(d_losses))\n",
        "        mean_g = float(np.mean(g_losses))\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # -------- PRINT PER-EPOCH OUTPUT --------\n",
        "        print(\n",
        "            f\"Epoch [{epoch:03d}/{epochs}] | \"\n",
        "            f\"D Loss: {mean_d:.4f} | \"\n",
        "            f\"G Loss: {mean_g:.4f} | \"\n",
        "            f\"Time: {elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        # -------- SAVE CHECKPOINTS --------\n",
        "        if epoch % 10 == 0:\n",
        "            dcgan.G.save(\n",
        "                os.path.join(checkpoint_dir, f\"G_epoch_{epoch:03d}.keras\")\n",
        "            )\n",
        "            dcgan.D.save(\n",
        "                os.path.join(checkpoint_dir, f\"D_epoch_{epoch:03d}.keras\")\n",
        "            )\n",
        "\n",
        "            # Latest models (easy reload)\n",
        "            dcgan.G.save(os.path.join(checkpoint_dir, \"G_latest.keras\"))\n",
        "            dcgan.D.save(os.path.join(checkpoint_dir, \"D_latest.keras\"))\n",
        "\n",
        "            print(f\"‚úÖ Checkpoints saved at epoch {epoch}\")\n",
        "\n",
        "        history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"d_loss\": mean_d,\n",
        "            \"g_loss\": mean_g,\n",
        "            \"time_sec\": elapsed\n",
        "        })\n",
        "\n",
        "    print(\"=\" * 75)\n",
        "    print(\"‚úÖ DCGAN TRAINING COMPLETED\")\n",
        "\n",
        "    history_df = pd.DataFrame(history)\n",
        "    history_df.to_csv(\n",
        "        os.path.join(checkpoint_dir, \"training_log.csv\"),\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    return history_df\n",
        "generator = build_generator(latent_dim=CONFIG[\"latent_dim\"])\n",
        "discriminator = build_discriminator(\n",
        "    img_size=CONFIG[\"img_size\"],\n",
        "    channels=CONFIG[\"channels\"]\n",
        ")\n",
        "\n",
        "dcgan = DCGAN(generator, discriminator, CONFIG)\n",
        "\n",
        "train_dataset = load_plantvillage_dataset(\n",
        "    img_size=CONFIG[\"img_size\"],\n",
        "    batch_size=CONFIG[\"batch_size\"]\n",
        ")\n",
        "\n",
        "history_df = train_dcgan(\n",
        "    dcgan=dcgan,\n",
        "    train_dataset=train_dataset,\n",
        "    epochs=CONFIG[\"epochs\"],\n",
        "    checkpoint_dir=CONFIG[\"checkpoint_dir\"]\n",
        ")\n",
        "\n",
        "history_df.tail()\n"
      ],
      "metadata": {
        "id": "WtfNHSxCOigC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 9: Inception Score (IS) ‚Äì COMPLETE SINGLE CELL\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1‚ç° Paths & parameters\n",
        "# ------------------------------------------------\n",
        "EVAL_DIR = r\"/content/drive/MyDrive/DCGANProject/evaluation\"\n",
        "CHECKPOINT_DIR = r\"/content/drive/MyDrive/DCGANProject/checkpoints\"\n",
        "DATASET_ROOT = r\"/content/drive/MyDrive/DCGANProject/ExampleDataDir/PartialPlantExamples\"\n",
        "os.makedirs(EVAL_DIR, exist_ok=True)\n",
        "\n",
        "N_EVAL = 256      # number of generated images\n",
        "SPLITS = 10       # standard value for IS\n",
        "INCEPTION_BATCH_SIZE = 16 # New parameter for batching InceptionV3 inference (Reduced further to avoid OOM)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2‚ç° Load trained Generator\n",
        "# ------------------------------------------------\n",
        "g_path = os.path.join(CHECKPOINT_DIR, \"G_latest.keras\")\n",
        "if not os.path.exists(g_path):\n",
        "    raise RuntimeError(\"‚ùå Generator not found. Train DCGAN first.\")\n",
        "\n",
        "generator = tf.keras.models.load_model(g_path, compile=False)\n",
        "latent_dim = generator.input_shape[1]\n",
        "\n",
        "print(\"‚úÖ Generator loaded\")\n",
        "print(\"Latent dimension:\", latent_dim)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3‚ç° Load InceptionV3 classifier (REQUIRED for IS)\n",
        "# ------------------------------------------------\n",
        "inception_model = InceptionV3(\n",
        "    include_top=True,      # ‚ÄÅ must be True (softmax output)\n",
        "    weights=\"imagenet\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ InceptionV3 classifier loaded\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4‚ç° Generate fake images\n",
        "# ------------------------------------------------\n",
        "def generate_fake_images(n):\n",
        "    z = tf.random.normal((n, latent_dim))\n",
        "    fake = generator(z, training=False)\n",
        "    fake = (fake + 1.0) / 2.0   # [-1,1] ‚Üí [0,1]\n",
        "    return fake\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5‚ç° Compute Inception Score (CORRECT)\n",
        "# ------------------------------------------------\n",
        "def compute_inception_score(images, splits=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    images: Tensor [N, H, W, 3] in range [0,1]\n",
        "    returns: (IS_mean, IS_std)\n",
        "    \"\"\"\n",
        "\n",
        "    # Predict class probabilities in batches to avoid OOM\n",
        "    all_preds = []\n",
        "    num_images = images.shape[0]\n",
        "    for i in range(0, num_images, batch_size):\n",
        "        batch_images = images[i:min(i + batch_size, num_images)]\n",
        "\n",
        "        # Resize for InceptionV3\n",
        "        batch_images = tf.image.resize(batch_images, (299, 299))\n",
        "\n",
        "        # Preprocess for InceptionV3\n",
        "        batch_images = preprocess_input(batch_images * 255.0)\n",
        "\n",
        "        preds = inception_model(batch_images, training=False).numpy()\n",
        "        all_preds.append(preds)\n",
        "\n",
        "    preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    # Safety normalization\n",
        "    preds = preds / np.sum(preds, axis=1, keepdims=True)\n",
        "\n",
        "    scores = []\n",
        "    N = preds.shape[0]\n",
        "    split_size = N // splits\n",
        "\n",
        "    for i in range(splits):\n",
        "        part = preds[i * split_size:(i + 1) * split_size]\n",
        "        p_y = np.mean(part, axis=0, keepdims=True)\n",
        "\n",
        "        kl = part * (np.log(part + 1e-10) - np.log(p_y + 1e-10))\n",
        "        kl = np.sum(kl, axis=1)\n",
        "\n",
        "        scores.append(np.exp(np.mean(kl)))\n",
        "\n",
        "    return float(np.mean(scores)), float(np.std(scores))\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6‚ç° RUN Inception Score\n",
        "# ------------------------------------------------\n",
        "fake_images = generate_fake_images(N_EVAL)\n",
        "\n",
        "is_mean, is_std = compute_inception_score(\n",
        "    fake_images,\n",
        "    splits=SPLITS,\n",
        "    batch_size=INCEPTION_BATCH_SIZE # Pass the batch size for inference\n",
        ")\n",
        "\n",
        "print(f\"üìä Inception Score = {is_mean:.3f} ¬± {is_std:.3f}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7‚ç° Save results\n",
        "# ------------------------------------------------\n",
        "df = pd.DataFrame([{\n",
        "    \"InceptionScore_mean\": is_mean,\n",
        "    \"InceptionScore_std\": is_std,\n",
        "    \"num_images\": N_EVAL\n",
        "}])\n",
        "\n",
        "df.to_csv(os.path.join(EVAL_DIR, \"inception_score.csv\"), index=False)\n",
        "\n",
        "print(\"‚úÖ Inception Score saved to evaluation/inception_score.csv\")"
      ],
      "metadata": {
        "id": "JC3G3ED8Okdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 10: Load Trained Generator / Discriminator\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_trained_models(checkpoint_dir, epoch=None):\n",
        "    \"\"\"\n",
        "    Loads trained Generator and Discriminator.\n",
        "    If epoch is None, loads the latest available model.\n",
        "    \"\"\"\n",
        "    if epoch is None:\n",
        "        g_path = os.path.join(checkpoint_dir, \"G_latest.keras\")\n",
        "        d_path = os.path.join(checkpoint_dir, \"D_latest.keras\")\n",
        "    else:\n",
        "        g_path = os.path.join(checkpoint_dir, f\"G_epoch_{epoch:03d}.keras\")\n",
        "        d_path = os.path.join(checkpoint_dir, f\"D_epoch_{epoch:03d}.keras\")\n",
        "\n",
        "    if not os.path.exists(g_path) or not os.path.exists(d_path):\n",
        "        raise FileNotFoundError(\"Checkpoint files not found.\")\n",
        "\n",
        "    generator = tf.keras.models.load_model(g_path, compile=False)\n",
        "    discriminator = tf.keras.models.load_model(d_path, compile=False)\n",
        "\n",
        "    print(\"‚úÖ Models loaded successfully\")\n",
        "    return generator, discriminator\n"
      ],
      "metadata": {
        "id": "KmEObfk-OmD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 11: Visualization (FIXED & SAFE)\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "FIG_DIR = r\"C:\\Users\\devar\\DCGANProject\\figures\"\n",
        "CHECKPOINT_DIR =  r\"C:\\Users\\devar\\DCGANProject\\checkpoints\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1Ô∏è‚É£ Load history safely\n",
        "# ------------------------------------------------\n",
        "if \"history_df\" not in globals():\n",
        "    log_path = os.path.join(CHECKPOINT_DIR, \"training_log.csv\")\n",
        "    if os.path.exists(log_path):\n",
        "        history_df = pd.read_csv(log_path)\n",
        "        print(\"‚úÖ Loaded training_log.csv\")\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            \"‚ùå history_df not found. Run Cell-8B (training) first.\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2Ô∏è‚É£ Load generator safely\n",
        "# ------------------------------------------------\n",
        "if \"generator\" not in globals():\n",
        "    g_path = os.path.join(CHECKPOINT_DIR, \"G_latest.keras\")\n",
        "    if os.path.exists(g_path):\n",
        "        generator = tf.keras.models.load_model(g_path, compile=False)\n",
        "        print(\"‚úÖ Loaded trained Generator\")\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            \"‚ùå Generator not found. Train model or load checkpoints.\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3Ô∏è‚É£ Plot Training Loss Curves\n",
        "# ------------------------------------------------\n",
        "def plot_training_losses(history_df):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(history_df[\"epoch\"], history_df[\"d_loss\"], label=\"Discriminator Loss\")\n",
        "    plt.plot(history_df[\"epoch\"], history_df[\"g_loss\"], label=\"Generator Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"DCGAN Training Losses\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIG_DIR, \"dcgan_loss_curve.png\"))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4Ô∏è‚É£ Generate & Display Synthetic Images\n",
        "# ------------------------------------------------\n",
        "def generate_and_show_images(generator, latent_dim, n=16):\n",
        "    noise = tf.random.normal((n, latent_dim))\n",
        "    fake_images = generator(noise, training=False)\n",
        "\n",
        "    # Rescale [-1,1] ‚Üí [0,1]\n",
        "    fake_images = (fake_images + 1.0) / 2.0\n",
        "\n",
        "    grid_size = int(np.sqrt(n))\n",
        "    plt.figure(figsize=(6, 6))\n",
        "\n",
        "    for i in range(n):\n",
        "        plt.subplot(grid_size, grid_size, i + 1)\n",
        "        plt.imshow(fake_images[i])\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Generated Crop Leaf Images\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIG_DIR, \"generated_samples.png\"))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5Ô∏è‚É£ Run Visualizations (FIXED)\n",
        "# ------------------------------------------------\n",
        "\n",
        "latent_dim = generator.input_shape[1]  # üîë infer automatically\n",
        "\n",
        "plot_training_losses(history_df)\n",
        "\n",
        "generate_and_show_images(\n",
        "    generator=generator,\n",
        "    latent_dim=latent_dim,\n",
        "    n=16\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "QweEBTFCOnnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 12: Inference (Save ALL Images Individually)\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Paths\n",
        "# ------------------------------------------------\n",
        "# Use paths from the global CONFIG for consistency\n",
        "CHECKPOINT_DIR = CONFIG[\"checkpoint_dir\"]\n",
        "OUTPUT_DIR = os.path.join(CONFIG[\"data_dir\"].split(\"/\")[0], CONFIG[\"data_dir\"].split(\"/\")[1], CONFIG[\"data_dir\"].split(\"/\")[2], CONFIG[\"data_dir\"].split(\"/\")[3], CONFIG[\"data_dir\"].split(\"/\")[4], \"inference_outputs\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1‚ç° Load trained Generator\n",
        "# ------------------------------------------------\n",
        "g_path = os.path.join(CHECKPOINT_DIR, \"G_latest.keras\")\n",
        "\n",
        "if not os.path.exists(g_path):\n",
        "    raise RuntimeError(\n",
        "        \"‚ùå Trained Generator not found. Run Cell-8B (training) first.\"\n",
        "    )\n",
        "\n",
        "generator = tf.keras.models.load_model(g_path, compile=False)\n",
        "print(\"‚úÖ Generator loaded successfully\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2‚ç° Infer latent dimension automatically\n",
        "# ------------------------------------------------\n",
        "latent_dim = generator.input_shape[1]\n",
        "print(\"Latent dimension:\", latent_dim)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3‚ç° Generate synthetic images\n",
        "# ------------------------------------------------\n",
        "def generate_images(generator, n=64):\n",
        "    print(\"Number of Images Generated =\", n)\n",
        "\n",
        "    noise = tf.random.normal((n, latent_dim))\n",
        "    fake_images = generator(noise, training=False)\n",
        "\n",
        "    # Rescale from [-1,1] ‚Üí [0,1]\n",
        "    fake_images = (fake_images + 1.0) / 2.0\n",
        "    return fake_images\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4‚ç° Save ALL images individually\n",
        "# ------------------------------------------------\n",
        "def save_all_images(images, output_dir, prefix=\"synthetic_leaf\"):\n",
        "    num_images = images.shape[0]\n",
        "\n",
        "    for i in range(num_images):\n",
        "        file_path = os.path.join(\n",
        "            output_dir, f\"{prefix}_{i+1:03d}.png\"\n",
        "        )\n",
        "        plt.imsave(file_path, images[i])\n",
        "\n",
        "    print(f\"‚úÖ Saved {num_images} images to '{output_dir}'\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5‚ç° RUN INFERENCE\n",
        "# ------------------------------------------------\n",
        "N_IMAGES = 64\n",
        "\n",
        "synthetic_images = generate_images(\n",
        "    generator=generator,\n",
        "    n=N_IMAGES\n",
        ")\n",
        "\n",
        "save_all_images(\n",
        "    images=synthetic_images,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    prefix=\"synthetic_leaf\"\n",
        ")\n",
        "\n",
        "print(\"üéâ Inference complete\")"
      ],
      "metadata": {
        "id": "yMQg0AekOp3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 13: Streamlit App ‚Äì Leaf Disease Image Generator\n",
        "# =====================================\n",
        "\n",
        "!pip install streamlit\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "st.set_page_config(page_title=\"Leaf Disease GAN\", layout=\"centered\")\n",
        "# ------------------------------------------------\n",
        "# Configuration\n",
        "# ------------------------------------------------\n",
        "CHECKPOINT_DIR = r\"/content/drive/MyDrive/DCGANProject/checkpoints\"\n",
        "OUTPUT_DIR = r\"/content/drive/MyDrive/DCGANProject/streamlit_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Load Generator (cached)\n",
        "# ------------------------------------------------\n",
        "@st.cache_resource\n",
        "def load_generator():\n",
        "    model_path = os.path.join(CHECKPOINT_DIR, \"G_latest.keras\")\n",
        "    if not os.path.exists(model_path):\n",
        "        st.error(\"‚ùå Generator model not found. Train DCGAN first.\")\n",
        "        st.stop()\n",
        "    return tf.keras.models.load_model(model_path, compile=False)\n",
        "\n",
        "\n",
        "generator = load_generator()\n",
        "latent_dim = generator.input_shape[1]\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Image generation function\n",
        "# ------------------------------------------------\n",
        "def generate_images(n):\n",
        "    z = tf.random.normal((n, latent_dim))\n",
        "    fake_images = generator(z, training=False)\n",
        "    fake_images = (fake_images + 1.0) / 2.0  # [-1,1] ‚Üí [0,1]\n",
        "    return fake_images.numpy()\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Streamlit UI\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "st.title(\"üå± Crop Leaf Disease Image Generator\")\n",
        "st.markdown(\n",
        "    \"Generate **synthetic crop leaf disease images** using a trained **DCGAN**.\"\n",
        ")\n",
        "\n",
        "num_images = st.slider(\n",
        "    \"Number of images to generate\",\n",
        "    min_value=1,\n",
        "    max_value=64,\n",
        "    value=16\n",
        ")\n",
        "\n",
        "generate_btn = st.button(\"Generate Images\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Generate & display images\n",
        "# ------------------------------------------------\n",
        "if generate_btn:\n",
        "    st.info(\"Generating images...\")\n",
        "    images = generate_images(num_images)\n",
        "\n",
        "    cols = st.columns(4)\n",
        "    for i, img in enumerate(images):\n",
        "        img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
        "        save_path = os.path.join(OUTPUT_DIR, f\"leaf_{i+1:03d}.png\")\n",
        "        img_pil.save(save_path)\n",
        "\n",
        "        cols[i % 4].image(img_pil, caption=f\"Image {i+1}\", use_container_width=True)\n",
        "\n",
        "    st.success(f\"‚úÖ {num_images} images generated and saved to `{OUTPUT_DIR}/`\")"
      ],
      "metadata": {
        "id": "5VxFkOdrOpu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 13: Streamlit App ‚Äì Leaf Disease Image Generator\n",
        "# =====================================\n",
        "\n",
        "# NOTE: To run this Streamlit app, you need to save this code to a .py file\n",
        "# and then execute it from the Colab terminal using `streamlit run <filename.py>`.\n",
        "# A tunneling service like `localtunnel` or `ngrok` is also needed to expose\n",
        "# the app to the internet from Colab.\n",
        "\n",
        "# We will handle the installation and running in subsequent steps.\n",
        "\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "st.set_page_config(page_title=\"Leaf Disease GAN\", layout=\"centered\")\n",
        "# ------------------------------------------------\n",
        "# Configuration\n",
        "# ------------------------------------------------\n",
        "CHECKPOINT_DIR = r\"/content/drive/MyDrive/DCGANProject/checkpoints\"\n",
        "OUTPUT_DIR = r\"/content/drive/MyDrive/DCGANProject/streamlit_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Load Generator (cached)\n",
        "# ------------------------------------------------\n",
        "@st.cache_resource\n",
        "def load_generator():\n",
        "    model_path = os.path.join(CHECKPOINT_DIR, \"G_latest.keras\")\n",
        "    if not os.path.exists(model_path):\n",
        "        st.error(\"‚ùå Generator model not found. Train DCGAN first.\")\n",
        "        st.stop()\n",
        "    return tf.keras.models.load_model(model_path, compile=False)\n",
        "\n",
        "\n",
        "generator = load_generator()\n",
        "latent_dim = generator.input_shape[1]\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Image generation function\n",
        "# ------------------------------------------------\n",
        "def generate_images(n):\n",
        "    z = tf.random.normal((n, latent_dim))\n",
        "    fake_images = generator(z, training=False)\n",
        "    fake_images = (fake_images + 1.0) / 2.0  # [-1,1] ‚Üí [0,1]\n",
        "    return fake_images.numpy()\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Streamlit UI\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "st.title(\"üå± Crop Leaf Disease Image Generator\")\n",
        "st.markdown(\n",
        "    \"Generate **synthetic crop leaf disease images** using a trained **DCGAN**.\"\n",
        ")\n",
        "\n",
        "num_images = st.slider(\n",
        "    \"Number of images to generate\",\n",
        "    min_value=1,\n",
        "    max_value=64,\n",
        "    value=16\n",
        ")\n",
        "\n",
        "generate_btn = st.button(\"Generate Images\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Generate & display images\n",
        "# ------------------------------------------------\n",
        "if generate_btn:\n",
        "    st.info(\"Generating images...\")\n",
        "    images = generate_images(num_images)\n",
        "\n",
        "    cols = st.columns(4)\n",
        "    for i, img in enumerate(images):\n",
        "        img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
        "        save_path = os.path.join(OUTPUT_DIR, f\"leaf_{i+1:03d}.png\")\n",
        "        img_pil.save(save_path)\n",
        "\n",
        "        cols[i % 4].image(img_pil, caption=f\"Image {i+1}\", use_container_width=True)\n",
        "\n",
        "    st.success(f\"‚úÖ {num_images} images generated and saved to `{OUTPUT_DIR}/`\")"
      ],
      "metadata": {
        "id": "56M5PWzZOuYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "RCOwfBqqOwtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "st.set_page_config(page_title=\"Leaf Disease GAN\", layout=\"centered\")\n",
        "# ------------------------------------------------\n",
        "# Configuration\n",
        "# ------------------------------------------------\n",
        "CHECKPOINT_DIR = r\"/content/drive/MyDrive/DCGANProject/checkpoints\"\n",
        "OUTPUT_DIR = r\"/content/drive/MyDrive/DCGANProject/streamlit_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Load Generator (cached)\n",
        "# ------------------------------------------------\n",
        "@st.cache_resource\n",
        "def load_generator():\n",
        "    model_path = os.path.join(CHECKPOINT_DIR, \"G_latest.keras\")\n",
        "    if not os.path.exists(model_path):\n",
        "        st.error(\"‚ùå Generator model not found. Train DCGAN first.\")\n",
        "        st.stop()\n",
        "    return tf.keras.models.load_model(model_path, compile=False)\n",
        "\n",
        "\n",
        "generator = load_generator()\n",
        "latent_dim = generator.input_shape[1]\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Image generation function\n",
        "# ------------------------------------------------\n",
        "def generate_images(n):\n",
        "    z = tf.random.normal((n, latent_dim))\n",
        "    fake_images = generator(z, training=False)\n",
        "    fake_images = (fake_images + 1.0) / 2.0  # [-1,1] ‚Üí [0,1]\n",
        "    return fake_images.numpy()\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Streamlit UI\n",
        "# ------------------------------------------------\n",
        "\n",
        "\n",
        "st.title(\"üå± Crop Leaf Disease Image Generator\")\n",
        "st.markdown(\n",
        "    \"Generate **synthetic crop leaf disease images** using a trained **DCGAN**.\"\n",
        ")\n",
        "\n",
        "num_images = st.slider(\n",
        "    \"Number of images to generate\",\n",
        "    min_value=1,\n",
        "    max_value=64,\n",
        "    value=16\n",
        ")\n",
        "\n",
        "generate_btn = st.button(\"Generate Images\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Generate & display images\n",
        "# ------------------------------------------------\n",
        "if generate_btn:\n",
        "    st.info(\"Generating images...\")\n",
        "    images = generate_images(num_images)\n",
        "\n",
        "    cols = st.columns(4)\n",
        "    for i, img in enumerate(images):\n",
        "        img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
        "        save_path = os.path.join(OUTPUT_DIR, f\"leaf_{i+1:03d}.png\")\n",
        "        img_pil.save(save_path)\n",
        "\n",
        "        cols[i % 4].image(img_pil, caption=f\"Image {i+1}\", use_container_width=True)\n",
        "\n",
        "    st.success(f\"‚úÖ {num_images} images generated and saved to `{OUTPUT_DIR}/`\")"
      ],
      "metadata": {
        "id": "ImE2Np0rOyRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import threading\n",
        "\n",
        "def run_streamlit():\n",
        "    # Run Streamlit in a separate process\n",
        "    process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.enableCORS\", \"false\", \"--server.enableXsrfProtection\", \"false\"])\n",
        "    print(\"Streamlit app is running on port 8501...\")\n",
        "    process.wait() # Keep the process alive\n",
        "\n",
        "def run_localtunnel():\n",
        "    # Give Streamlit a moment to start\n",
        "    time.sleep(10)\n",
        "    print(\"Starting localtunnel...\")\n",
        "    # Expose the Streamlit port 8501 via localtunnel\n",
        "    subprocess.run([\"lt\", \"--port\", \"8501\"])\n",
        "\n",
        "# Start Streamlit in a separate thread\n",
        "streamlit_thread = threading.Thread(target=run_streamlit)\n",
        "streamlit_thread.start()\n",
        "\n",
        "# Start localtunnel in the main thread (or another thread if preferred)\n",
        "run_localtunnel()"
      ],
      "metadata": {
        "id": "mvjhqbTXOz-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Cell 15: Monitoring & Versioning\n",
        "# =====================================\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Configuration\n",
        "# ------------------------------------------------\n",
        "REGISTRY_DIR = r\"/content/drive/MyDrive/DCGANProject/model_registry\"\n",
        "LOG_DIR = r\"/content/drive/MyDrive/DCGANProject/monitoring_logs\"\n",
        "EVAL_DIR = r\"/content/drive/MyDrive/DCGANProject/evaluation\"\n",
        "CHECKPOINT_DIR = r\"/content/drive/MyDrive/DCGANProject/checkpoints\"\n",
        "\n",
        "os.makedirs(REGISTRY_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "REGISTRY_FILE = os.path.join(REGISTRY_DIR, \"model_registry.yaml\")\n",
        "USAGE_LOG_FILE = os.path.join(LOG_DIR, \"usage_log.csv\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1Ô∏è‚É£ Initialize Model Registry (if not exists)\n",
        "# ------------------------------------------------\n",
        "if not os.path.exists(REGISTRY_FILE):\n",
        "    with open(REGISTRY_FILE, \"w\") as f:\n",
        "        yaml.safe_dump({\"models\": []}, f)\n",
        "\n",
        "print(\"‚úÖ Model registry ready\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2Ô∏è‚É£ Register a new trained model version\n",
        "# ------------------------------------------------\n",
        "def register_model(\n",
        "    model_name,\n",
        "    version,\n",
        "    generator_path,\n",
        "    training_config,\n",
        "    metrics=None\n",
        "):\n",
        "    with open(REGISTRY_FILE, \"r\") as f:\n",
        "        registry = yaml.safe_load(f)\n",
        "\n",
        "    entry = {\n",
        "        \"model_name\": model_name,\n",
        "        \"version\": version,\n",
        "        \"generator_path\": generator_path,\n",
        "        \"registered_at\": datetime.now().isoformat(),\n",
        "        \"training_config\": training_config,\n",
        "        \"metrics\": metrics or {}\n",
        "    }\n",
        "\n",
        "    registry[\"models\"].append(entry)\n",
        "\n",
        "    with open(REGISTRY_FILE, \"w\") as f:\n",
        "        yaml.safe_dump(registry, f)\n",
        "\n",
        "    print(f\"‚úÖ Model {model_name} v{version} registered\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3Ô∏è‚É£ Load latest evaluation metrics\n",
        "# ------------------------------------------------\n",
        "def load_latest_metrics():\n",
        "    metrics = {}\n",
        "\n",
        "    fid_file = os.path.join(EVAL_DIR, \"gan_metrics.csv\")\n",
        "    is_file = os.path.join(EVAL_DIR, \"inception_score.csv\")\n",
        "\n",
        "    if os.path.exists(fid_file):\n",
        "        fid_df = pd.read_csv(fid_file)\n",
        "        metrics[\"FID\"] = float(fid_df.iloc[-1][\"FID\"])\n",
        "\n",
        "    if os.path.exists(is_file):\n",
        "        is_df = pd.read_csv(is_file)\n",
        "        metrics[\"IS_mean\"] = float(is_df.iloc[-1][\"InceptionScore_mean\"])\n",
        "        metrics[\"IS_std\"] = float(is_df.iloc[-1][\"InceptionScore_std\"])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4Ô∏è‚É£ Register current model automatically\n",
        "# ------------------------------------------------\n",
        "MODEL_NAME = \"DCGAN_Leaf_Disease\"\n",
        "MODEL_VERSION = \"1.0\"\n",
        "\n",
        "training_config = {\n",
        "    \"latent_dim\": 100,\n",
        "    \"image_size\": 64,\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\": 0.0002,\n",
        "    \"beta_1\": 0.5,\n",
        "    \"epochs\": 100\n",
        "}\n",
        "\n",
        "metrics = load_latest_metrics()\n",
        "\n",
        "register_model(\n",
        "    model_name=MODEL_NAME,\n",
        "    version=MODEL_VERSION,\n",
        "    generator_path=os.path.join(CHECKPOINT_DIR, \"G_latest.keras\"),\n",
        "    training_config=training_config,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5Ô∏è‚É£ Usage logging (Streamlit / API)\n",
        "# ------------------------------------------------\n",
        "def log_usage(\n",
        "    source=\"streamlit\",\n",
        "    num_images=0,\n",
        "    crop=None,\n",
        "    disease=None\n",
        "):\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"source\": source,\n",
        "        \"num_images\": num_images,\n",
        "        \"crop\": crop,\n",
        "        \"disease\": disease\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(USAGE_LOG_FILE):\n",
        "        df = pd.DataFrame([log_entry])\n",
        "    else:\n",
        "        df = pd.read_csv(USAGE_LOG_FILE)\n",
        "        df = pd.concat([df, pd.DataFrame([log_entry])], ignore_index=True)\n",
        "\n",
        "    df.to_csv(USAGE_LOG_FILE, index=False)\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6Ô∏è‚É£ Example usage log (demo)\n",
        "# ------------------------------------------------\n",
        "log_usage(\n",
        "    source=\"streamlit\",\n",
        "    num_images=16,\n",
        "    crop=\"tomato\",\n",
        "    disease=\"blight\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Usage logged\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7Ô∏è‚É£ Read monitoring summary\n",
        "# ------------------------------------------------\n",
        "def monitoring_summary():\n",
        "    if not os.path.exists(USAGE_LOG_FILE):\n",
        "        print(\"No usage logs yet.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(USAGE_LOG_FILE)\n",
        "    print(\"\\nüìä Monitoring Summary\")\n",
        "    print(\"---------------------\")\n",
        "    print(\"Total requests :\", len(df))\n",
        "    print(\"Total images generated :\", df[\"num_images\"].sum())\n",
        "    print(\"Most requested crop :\", df[\"crop\"].mode().values[0])\n",
        "    print(\"Most requested disease :\", df[\"disease\"].mode().values[0])\n",
        "\n",
        "\n",
        "monitoring_summary()\n"
      ],
      "metadata": {
        "id": "YpfFGrPHO1eo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}